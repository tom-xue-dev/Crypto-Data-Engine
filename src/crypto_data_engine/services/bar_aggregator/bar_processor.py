# """
# Bar ç”ŸæˆæœåŠ¡ - å°† Tick æ•°æ®è½¬æ¢ä¸ºå„ç§ç±»å‹çš„ Bar æ•°æ®
# """
# import os
# import pandas as pd
# from pathlib import Path
# from datetime import datetime, timezone
# from typing import List, Dict, Any, Optional
# from tqdm import tqdm
# import logging
#
# from crypto_data_engine.services.tick_data_scraper.app.bar_constructor import BarConstructor
#
# logger = logging.getLogger(__name__)
# class BarProcessorContext:
#     """Bar å¤„ç†ä¸Šä¸‹æ–‡"""
#     def __init__(self, config: Dict[str, Any]):
#         self.raw_data_dir = config['raw_data_dir']  # tick æ•°æ®è¾“å…¥ç›®å½•
#         self.output_dir = config['output_dir']  # bar æ•°æ®è¾“å‡ºç›®å½•
#         self.bar_type = config.get('bar_type', 'volume_bar')
#         self.threshold = config.get('threshold', 10000000)
#         self.process_num_limit = config.get('process_num_limit', 4)
#         self.suffix_filter = config.get('suffix_filter', None)
#         self.adaptive = config.get('adaptive', False)
#         self.sample_days = config.get('sample_days', 1)
#         self.target_bars = config.get('target_bars', 300)
#         self.batch_size = config.get('batch_size', 10)
#
#
# class BarProcessor:
#     """Bar æ•°æ®å¤„ç†å™¨"""
#
#     def __init__(self, context: BarProcessorContext):
#         self.context = context
#         self.processed_files = set()
#
#     def run_bar_generation_pipeline(self, config: Dict[str, Any]):
#         """è¿è¡Œ Bar ç”Ÿæˆæµæ°´çº¿"""
#         try:
#             logger.info(f"ğŸš€ å¼€å§‹ Bar ç”Ÿæˆæµæ°´çº¿")
#             logger.info(f"ğŸ“Š Bar ç±»å‹: {self.context.bar_type}")
#             logger.info(f"ğŸ“‚ è¾“å…¥ç›®å½•: {self.context.raw_data_dir}")
#             logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.context.output_dir}")
#
#             # è·å–å¾…å¤„ç†çš„æ–‡ä»¶
#             symbols = self._get_symbols_to_process(config)
#
#             if not symbols:
#                 logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„æ•°æ®æ–‡ä»¶")
#                 return {"status": "completed", "processed": 0, "message": "No files to process"}
#
#             # åˆ›å»ºè¾“å‡ºç›®å½•
#             os.makedirs(self.context.output_dir, exist_ok=True)
#
#             # å¤„ç†æ¯ä¸ªäº¤æ˜“å¯¹
#             total_processed = 0
#             results = []
#
#             for symbol in tqdm(symbols, desc="Processing symbols"):
#                 try:
#                     result = self._process_symbol(symbol)
#                     if result:
#                         results.append(result)
#                         total_processed += 1
#                         logger.info(f"âœ… {symbol} å¤„ç†å®Œæˆ: {result['bars_count']} bars")
#                 except Exception as e:
#                     logger.error(f"âŒ {symbol} å¤„ç†å¤±è´¥: {str(e)}")
#
#             logger.info(f"ğŸ‰ Bar ç”Ÿæˆå®Œæˆï¼å…±å¤„ç† {total_processed} ä¸ªäº¤æ˜“å¯¹")
#
#             return {
#                 "status": "completed",
#                 "processed": total_processed,
#                 "total_symbols": len(symbols),
#                 "results": results,
#                 "bar_type": self.context.bar_type,
#                 "threshold": self.context.threshold
#             }
#
#         except Exception as e:
#             logger.error(f"âŒ Bar ç”Ÿæˆæµæ°´çº¿å¤±è´¥: {str(e)}")
#             raise
#
#     def _get_symbols_to_process(self, config: Dict[str, Any]) -> List[str]:
#         """è·å–éœ€è¦å¤„ç†çš„äº¤æ˜“å¯¹åˆ—è¡¨"""
#         raw_data_path = Path(self.context.raw_data_dir)
#
#         if not raw_data_path.exists():
#             logger.error(f"âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {raw_data_path}")
#             return []
#
#         # è·å–æŒ‡å®šçš„äº¤æ˜“å¯¹æˆ–è‡ªåŠ¨å‘ç°
#         if 'symbols' in config and config['symbols']:
#             symbols = config['symbols']
#         else:
#             # è‡ªåŠ¨å‘ç°ç›®å½•ä¸‹çš„äº¤æ˜“å¯¹
#             symbols = [d.name for d in raw_data_path.iterdir()
#                        if d.is_dir() and not d.name.startswith('.')]
#
#         # åº”ç”¨åç¼€è¿‡æ»¤å™¨
#         if self.context.suffix_filter:
#             symbols = [s for s in symbols if s.endswith(self.context.suffix_filter)]
#
#         logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(symbols)} ä¸ªå¾…å¤„ç†äº¤æ˜“å¯¹: {symbols[:10]}...")
#         return symbols
#
#     def _process_symbol(self, symbol: str) -> Optional[Dict[str, Any]]:
#         """å¤„ç†å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®"""
#         symbol_path = Path(self.context.raw_data_dir) / symbol
#
#         if not symbol_path.exists() or not symbol_path.is_dir():
#             logger.warning(f"âš ï¸  äº¤æ˜“å¯¹ç›®å½•ä¸å­˜åœ¨: {symbol_path}")
#             return None
#
#         # è·å–è¯¥äº¤æ˜“å¯¹çš„æ‰€æœ‰ parquet æ–‡ä»¶
#         parquet_files = list(symbol_path.glob("*.parquet"))
#
#         if not parquet_files:
#             logger.warning(f"âš ï¸  {symbol} ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° parquet æ–‡ä»¶")
#             return None
#
#         # æ’åºæ–‡ä»¶ä»¥ç¡®ä¿æ—¶é—´é¡ºåº
#         parquet_files.sort()
#
#         logger.info(f"ğŸ“Š {symbol}: æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶")
#
#         # ä½¿ç”¨ BarConstructor å¤„ç†æ•°æ®
#         constructor = BarConstructor(
#             folder_path=parquet_files,
#             threshold=self.context.threshold,
#             bar_type=self.context.bar_type
#         )
#
#         # ç”Ÿæˆ Bar æ•°æ®
#         bars_df = constructor.process_asset_data()
#
#         if bars_df is None or bars_df.empty:
#             logger.warning(f"âš ï¸  {symbol} ç”Ÿæˆçš„ Bar æ•°æ®ä¸ºç©º")
#             return None
#
#         # ä¿å­˜ç»“æœ
#         output_file = self._save_bars(symbol, bars_df)
#
#         return {
#             "symbol": symbol,
#             "bars_count": len(bars_df),
#             "output_file": str(output_file),
#             "bar_type": self.context.bar_type,
#             "threshold": self.context.threshold,
#             "processed_at": datetime.now().isoformat()
#         }
#
#     def _save_bars(self, symbol: str, bars_df: pd.DataFrame) -> Path:
#         """ä¿å­˜ Bar æ•°æ®"""
#         # åˆ›å»ºäº¤æ˜“å¯¹ä¸“ç”¨è¾“å‡ºç›®å½•
#         symbol_output_dir = Path(self.context.output_dir) / symbol
#         symbol_output_dir.mkdir(parents=True, exist_ok=True)
#
#         # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         output_file = symbol_output_dir / f"{symbol}_{self.context.bar_type}_{self.context.threshold}_{timestamp}.parquet"
#
#         # ä¿å­˜ä¸º parquet æ ¼å¼
#         bars_df.to_parquet(output_file, compression="brotli", index=False)
#
#         logger.info(f"ğŸ’¾ {symbol} Bar æ•°æ®å·²ä¿å­˜: {output_file}")
#         return output_file
#
#     def get_processing_stats(self) -> Dict[str, Any]:
#         """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
#         return {
#             "processed_files": len(self.processed_files),
#             "context": {
#                 "bar_type": self.context.bar_type,
#                 "threshold": self.context.threshold,
#                 "raw_data_dir": self.context.raw_data_dir,
#                 "output_dir": self.context.output_dir
#             }
#         }
#
#
# def run_simple_bar_generation(
#         exchange_name: str = "binance",
#         symbols: Optional[List[str]] = None,
#         bar_type: str = "volume_bar",
#         threshold: int = 10000000,
#         raw_data_dir: str = "./data/tick_data",
#         output_dir: str = "./data/bar_data"
# ) -> Dict[str, Any]:
#     """
#     ç®€åŒ–çš„ Bar ç”Ÿæˆå‡½æ•° - ç±»ä¼¼äº run_simple_download
#     """
#
#     logger.info(f"\nğŸš€ å¼€å§‹ {exchange_name.upper()} Bar ç”Ÿæˆ")
#     logger.info(f"ğŸ“Š Bar ç±»å‹: {bar_type}")
#     logger.info(f"ğŸ¯ é˜ˆå€¼: {threshold}")
#
#     # æ„å»ºé…ç½®
#     config = {
#         'raw_data_dir': f"{raw_data_dir}/{exchange_name}",
#         'output_dir': f"{output_dir}/{exchange_name}",
#         'bar_type': bar_type,
#         'threshold': threshold,
#         'symbols': symbols,
#         'process_num_limit': 4,
#         'batch_size': 10
#     }
#
#     try:
#         # åˆ›å»ºå¤„ç†ä¸Šä¸‹æ–‡å’Œå¤„ç†å™¨
#         context = BarProcessorContext(config)
#         backend_processor = BarProcessor(context)
#
#         # è¿è¡Œ Bar ç”Ÿæˆæµæ°´çº¿
#         result = backend_processor.run_bar_generation_pipeline(config)
#
#         logger.info(f"\nğŸ‰ {exchange_name.upper()} Bar ç”Ÿæˆå®Œæˆï¼")
#         logger.info(f"ğŸ“Š å¤„ç†ç»“æœ: {result}")
#
#         return result
#
#     except Exception as e:
#         logger.error(f"âŒ Bar ç”Ÿæˆå¤±è´¥: {e}")
#         raise